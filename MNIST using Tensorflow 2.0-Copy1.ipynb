{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network for MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tensorflow 2.0 to classify the MNIST data set of 70000 images of 28x28 pixels of handwritten digits per image.\n",
    "The gols is to wite an algorithm to detect which digit is written. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target)\n",
    "mnist_train, mnist_test= mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "#number of validation samples\n",
    "num_validation_samples=0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples=tf.cast(num_validation_samples,tf.int64)\n",
    "\n",
    "#number of test_samples\n",
    "num_test_samples=mnist_info.splits['test'].num_examples\n",
    "num_test_samples=tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "#scale the data\n",
    "def scale(image,label):\n",
    "    # to make sure the value is a float\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255\n",
    "    image/=255.\n",
    "    return image,label\n",
    "\n",
    "# .map() allows to apply a custom transformation to a given dataset\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "# .take() method to take that many samples\n",
    "validation_data = scaled_train_and_validation_data.take(num_validation_samples)\n",
    "# .skip() as many samples as there are in the validation dataset\n",
    "train_data = scaled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE=100\n",
    "train_data=train_data.batch(BATCH_SIZE)\n",
    "validation_data=validation_data.batch(num_validation_samples)\n",
    "test_data=test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets= next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=200\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            \n",
    "                           \n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')                            \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "540/540 - 7s - loss: 0.2639 - accuracy: 0.9239 - val_loss: 0.1426 - val_accuracy: 0.9572\n",
      "Epoch 2/6\n",
      "540/540 - 3s - loss: 0.1027 - accuracy: 0.9698 - val_loss: 0.1299 - val_accuracy: 0.9638\n",
      "Epoch 3/6\n",
      "540/540 - 4s - loss: 0.0667 - accuracy: 0.9799 - val_loss: 0.1193 - val_accuracy: 0.9650\n",
      "Epoch 4/6\n",
      "540/540 - 4s - loss: 0.0484 - accuracy: 0.9852 - val_loss: 0.1095 - val_accuracy: 0.9682\n",
      "Epoch 5/6\n",
      "540/540 - 4s - loss: 0.0380 - accuracy: 0.9879 - val_loss: 0.1110 - val_accuracy: 0.9702\n",
      "Epoch 6/6\n",
      "540/540 - 3s - loss: 0.0309 - accuracy: 0.9895 - val_loss: 0.1297 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a80b88fc70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 6\n",
    "model.fit(train_data,epochs=NUM_EPOCHS, validation_data=(validation_inputs,validation_targets),verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 630ms/step - loss: 0.1299 - accuracy: 0.9655\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy= model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.13. Test accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "print('test loss: {0:.2f}. Test accuracy: {1:.2f}%' .format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
